/-
Copyright (c) 2023 Eric Wieser. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Eric Wieser
-/
import Mathlib.Data.Int.Order.Units
import Mathlib.Data.ZMod.IntUnitsPower
import Mathlib.RingTheory.TensorProduct.Basic
import Mathlib.LinearAlgebra.DirectSum.TensorProduct
import Mathlib.Algebra.DirectSum.Algebra

/-!
# Graded tensor products over graded algebras

The graded tensor product $A \hat\otimes_R B$ is imbued with a multiplication defined on homogeneous
tensors by:

$$(a \otimes b) \cdot (a' \otimes b') = (-1)^{\deg a' \deg b} (a \cdot a') \otimes (b \cdot b')$$

where $A$ and $B$ are algebras graded by `â„•`, `â„¤`, or `ZMod 2` (or more generally, any index
that satisfies `Module Î¹ (Additive â„¤Ë£)`).

The results for internally-graded algebras (via `GradedAlgebra`) are elsewhere, as is the type
`GradedTensorProduct`.

## Main results

* `TensorProduct.gradedComm`: the symmetric braiding operator on the tensor product of
  externally-graded rings.
* `TensorProduct.gradedMul`: the previously-described multiplication on externally-graded rings, as
  a bilinear map.

## Implementation notes

Rather than implementing the multiplication directly as above, we first implement the canonical
non-trivial braiding sending $a \otimes b$ to $(-1)^{\deg a' \deg b} (b \otimes a)$, as the
multiplication follows trivially from this after some point-free nonsense.

## References

* https://math.stackexchange.com/q/202718/1896
* [*Algebra I*, Bourbaki : Chapter III, Â§4.7, example (2)][bourbaki1989]

-/

open scoped TensorProduct DirectSum

variable {R Î¹ : Type*}

namespace TensorProduct

variable [CommSemiring Î¹] [Module Î¹ (Additive â„¤Ë£)] [DecidableEq Î¹]
variable (ğ’œ : Î¹ â†’ Type*) (â„¬ : Î¹ â†’ Type*)
variable [CommRing R]
variable [âˆ€ i, AddCommGroup (ğ’œ i)] [âˆ€ i, AddCommGroup (â„¬ i)]
variable [âˆ€ i, Module R (ğ’œ i)] [âˆ€ i, Module R (â„¬ i)]

-- this helps with performance
instance (i : Î¹ Ã— Î¹) : Module R (ğ’œ (Prod.fst i) âŠ—[R] â„¬ (Prod.snd i)) :=
  TensorProduct.leftModule

open DirectSum (lof)

variable (R)

section gradedComm

local notation "ğ’œâ„¬" => (fun i : Î¹ Ã— Î¹ => ğ’œ (Prod.fst i) âŠ—[R] â„¬ (Prod.snd i))
local notation "â„¬ğ’œ" => (fun i : Î¹ Ã— Î¹ => â„¬ (Prod.fst i) âŠ—[R] ğ’œ (Prod.snd i))

/-- Auxliary construction used to build `TensorProduct.gradedComm`.

This operates on direct sums of tensors instead of tensors of direct sums. -/
def gradedCommAux : DirectSum _ ğ’œâ„¬ â†’â‚—[R] DirectSum _ â„¬ğ’œ := by
  refine DirectSum.toModule R _ _ fun i => ?_
  have o := DirectSum.lof R _ â„¬ğ’œ i.swap
  have s : â„¤Ë£ := ((-1 : â„¤Ë£)^(i.1* i.2 : Î¹) : â„¤Ë£)
  exact (s â€¢ o) âˆ˜â‚— (TensorProduct.comm R _ _).toLinearMap

@[simp]
theorem gradedCommAux_lof_tmul (i j : Î¹) (a : ğ’œ i) (b : â„¬ j) :
    gradedCommAux R ğ’œ â„¬ (lof R _ ğ’œâ„¬ (i, j) (a âŠ—â‚œ b)) =
      (-1 : â„¤Ë£)^(j * i) â€¢ lof R _ â„¬ğ’œ (j, i) (b âŠ—â‚œ a) := by
  rw [gradedCommAux]
  simp [mul_comm i j]

@[simp]
theorem gradedCommAux_comp_gradedCommAux :
    gradedCommAux R ğ’œ â„¬ âˆ˜â‚— gradedCommAux R â„¬ ğ’œ = LinearMap.id := by
  ext i a b
  dsimp
  rw [gradedCommAux_lof_tmul, LinearMap.map_smul_of_tower, gradedCommAux_lof_tmul, smul_smul,
    mul_comm i.2 i.1, Int.units_mul_self, one_smul]

/-- The braiding operation for tensor products of externally `Î¹`-graded algebras.

This sends $a âŠ— b$ to $(-1)^{\deg a' \deg b} (b âŠ— a)$. -/
def gradedComm :
    (â¨ i, ğ’œ i) âŠ—[R] (â¨ i, â„¬ i) â‰ƒâ‚—[R] (â¨ i, â„¬ i) âŠ—[R] (â¨ i, ğ’œ i) := by
  refine TensorProduct.directSum R R ğ’œ â„¬ â‰ªâ‰«â‚— ?_ â‰ªâ‰«â‚— (TensorProduct.directSum R R â„¬ ğ’œ).symm
  exact LinearEquiv.ofLinear (gradedCommAux _ _ _) (gradedCommAux _ _ _)
    (gradedCommAux_comp_gradedCommAux _ _ _) (gradedCommAux_comp_gradedCommAux _ _ _)

/-- The braiding is symmetric. -/
@[simp]
theorem gradedComm_symm : (gradedComm R ğ’œ â„¬).symm = gradedComm R â„¬ ğ’œ := by
  rw [gradedComm, gradedComm, LinearEquiv.trans_symm, LinearEquiv.symm_symm]
  ext
  rfl

theorem gradedComm_of_tmul_of (i j : Î¹) (a : ğ’œ i) (b : â„¬ j) :
    gradedComm R ğ’œ â„¬ (lof R _ ğ’œ i a âŠ—â‚œ lof R _ â„¬ j b) =
      (-1 : â„¤Ë£)^(j * i) â€¢ (lof R _ â„¬ _ b âŠ—â‚œ lof R _ ğ’œ _ a) := by
  rw [gradedComm]
  dsimp only [LinearEquiv.trans_apply, LinearEquiv.ofLinear_apply]
  rw [TensorProduct.directSum_lof_tmul_lof, gradedCommAux_lof_tmul, Units.smul_def,
    -- Note: https://github.com/leanprover-community/mathlib4/pull/8386 specialized `map_smul` to `LinearEquiv.map_smul` to avoid timeouts.
    â† Int.cast_smul_eq_zsmul R, LinearEquiv.map_smul, TensorProduct.directSum_symm_lof_tmul,
    Int.cast_smul_eq_zsmul, â† Units.smul_def]

theorem gradedComm_tmul_of_zero (a : â¨ i, ğ’œ i) (b : â„¬ 0) :
    gradedComm R ğ’œ â„¬ (a âŠ—â‚œ lof R _ â„¬ 0 b) = lof R _ â„¬ _ b âŠ—â‚œ a := by
  suffices
    (gradedComm R ğ’œ â„¬).toLinearMap âˆ˜â‚—
        (TensorProduct.mk R (â¨ i, ğ’œ i) (â¨ i, â„¬ i)).flip (lof R _ â„¬ 0 b) =
      TensorProduct.mk R _ _ (lof R _ â„¬ 0 b) from
    DFunLike.congr_fun this a
  ext i a
  dsimp
  rw [gradedComm_of_tmul_of, zero_mul, uzpow_zero, one_smul]

theorem gradedComm_of_zero_tmul (a : ğ’œ 0) (b : â¨ i, â„¬ i) :
    gradedComm R ğ’œ â„¬ (lof R _ ğ’œ 0 a âŠ—â‚œ b) = b âŠ—â‚œ lof R _ ğ’œ _ a := by
  suffices
    (gradedComm R ğ’œ â„¬).toLinearMap âˆ˜â‚— (TensorProduct.mk R (â¨ i, ğ’œ i) (â¨ i, â„¬ i)) (lof R _ ğ’œ 0 a) =
      (TensorProduct.mk R _ _).flip (lof R _ ğ’œ 0 a) from
    DFunLike.congr_fun this b
  ext i b
  dsimp
  rw [gradedComm_of_tmul_of, mul_zero, uzpow_zero, one_smul]

theorem gradedComm_tmul_one [GradedMonoid.GOne â„¬] (a : â¨ i, ğ’œ i) :
    gradedComm R ğ’œ â„¬ (a âŠ—â‚œ 1) = 1 âŠ—â‚œ a :=
  gradedComm_tmul_of_zero _ _ _ _ _

theorem gradedComm_one_tmul [GradedMonoid.GOne ğ’œ] (b : â¨ i, â„¬ i) :
    gradedComm R ğ’œ â„¬ (1 âŠ—â‚œ b) = b âŠ—â‚œ 1 :=
  gradedComm_of_zero_tmul _ _ _ _ _

@[simp]
theorem gradedComm_one [DirectSum.GSemiring ğ’œ] [DirectSum.GSemiring â„¬] : gradedComm R ğ’œ â„¬ 1 = 1 :=
  gradedComm_one_tmul _ _ _ _

theorem gradedComm_tmul_algebraMap [DirectSum.GSemiring â„¬] [DirectSum.GAlgebra R â„¬]
    (a : â¨ i, ğ’œ i) (r : R) :
    gradedComm R ğ’œ â„¬ (a âŠ—â‚œ algebraMap R _ r) = algebraMap R _ r âŠ—â‚œ a :=
  gradedComm_tmul_of_zero _ _ _ _ _

theorem gradedComm_algebraMap_tmul [DirectSum.GSemiring ğ’œ] [DirectSum.GAlgebra R ğ’œ]
    (r : R) (b : â¨ i, â„¬ i) :
    gradedComm R ğ’œ â„¬ (algebraMap R _ r âŠ—â‚œ b) = b âŠ—â‚œ algebraMap R _ r :=
  gradedComm_of_zero_tmul _ _ _ _ _

theorem gradedComm_algebraMap [DirectSum.GSemiring ğ’œ] [DirectSum.GSemiring â„¬]
    [DirectSum.GAlgebra R ğ’œ] [DirectSum.GAlgebra R â„¬] (r : R) :
    gradedComm R ğ’œ â„¬ (algebraMap R _ r) = algebraMap R _ r :=
  (gradedComm_algebraMap_tmul R ğ’œ â„¬ r 1).trans (Algebra.TensorProduct.algebraMap_apply' r).symm

end gradedComm

variable [DirectSum.GRing ğ’œ] [DirectSum.GRing â„¬]
variable [DirectSum.GAlgebra R ğ’œ] [DirectSum.GAlgebra R â„¬]

open TensorProduct (assoc map) in
/-- The multiplication operation for tensor products of externally `Î¹`-graded algebras. -/
noncomputable irreducible_def gradedMul :
    letI AB := DirectSum _ ğ’œ âŠ—[R] DirectSum _ â„¬
    letI : Module R AB := TensorProduct.leftModule
    AB â†’â‚—[R] AB â†’â‚—[R] AB := by
  refine TensorProduct.curry ?_
  refine map (LinearMap.mul' R (â¨ i, ğ’œ i)) (LinearMap.mul' R (â¨ i, â„¬ i)) âˆ˜â‚— ?_
  refine (assoc R _ _ _).symm.toLinearMap âˆ˜â‚— .lTensor _ ?_ âˆ˜â‚— (assoc R _ _ _).toLinearMap
  refine (assoc R _ _ _).toLinearMap âˆ˜â‚— .rTensor _ ?_ âˆ˜â‚— (assoc R _ _ _).symm.toLinearMap
  exact (gradedComm _ _ _).toLinearMap

theorem tmul_of_gradedMul_of_tmul (jâ‚ iâ‚‚ : Î¹)
    (aâ‚ : â¨ i, ğ’œ i) (bâ‚ : â„¬ jâ‚) (aâ‚‚ : ğ’œ iâ‚‚) (bâ‚‚ : â¨ i, â„¬ i) :
    gradedMul R ğ’œ â„¬ (aâ‚ âŠ—â‚œ lof R _ â„¬ jâ‚ bâ‚) (lof R _ ğ’œ iâ‚‚ aâ‚‚ âŠ—â‚œ bâ‚‚) =
      (-1 : â„¤Ë£)^(jâ‚ * iâ‚‚) â€¢ ((aâ‚ * lof R _ ğ’œ _ aâ‚‚) âŠ—â‚œ (lof R _ â„¬ _ bâ‚ * bâ‚‚)) := by
  rw [gradedMul]
  dsimp only [curry_apply, LinearMap.coe_comp, LinearEquiv.coe_coe, Function.comp_apply, assoc_tmul,
    map_tmul, LinearMap.id_coe, id_eq, assoc_symm_tmul, LinearMap.rTensor_tmul,
    LinearMap.lTensor_tmul]
  rw [mul_comm jâ‚ iâ‚‚, gradedComm_of_tmul_of]
  -- the tower smul lemmas elaborate too slowly
  rw [Units.smul_def, Units.smul_def, â† Int.cast_smul_eq_zsmul R, â† Int.cast_smul_eq_zsmul R]
  -- Note: https://github.com/leanprover-community/mathlib4/pull/8386 had to specialize `map_smul` to avoid timeouts.
  rw [â† smul_tmul', LinearEquiv.map_smul, tmul_smul, LinearEquiv.map_smul, LinearMap.map_smul]
  dsimp

variable {R}

theorem algebraMap_gradedMul (r : R) (x : (â¨ i, ğ’œ i) âŠ—[R] (â¨ i, â„¬ i)) :
    gradedMul R ğ’œ â„¬ (algebraMap R _ r âŠ—â‚œ 1) x = r â€¢ x := by
  suffices gradedMul R ğ’œ â„¬ (algebraMap R _ r âŠ—â‚œ 1) = DistribMulAction.toLinearMap R _ r by
    exact DFunLike.congr_fun this x
  ext ia a ib b
  dsimp
  erw [tmul_of_gradedMul_of_tmul]
  rw [zero_mul, uzpow_zero, one_smul, smul_tmul']
  erw [one_mul, _root_.Algebra.smul_def]

theorem one_gradedMul (x : (â¨ i, ğ’œ i) âŠ—[R] (â¨ i, â„¬ i)) :
    gradedMul R ğ’œ â„¬ 1 x = x := by
  -- Note: https://github.com/leanprover-community/mathlib4/pull/8386 had to specialize `map_one` to avoid timeouts.
  simpa only [RingHom.map_one, one_smul] using algebraMap_gradedMul ğ’œ â„¬ 1 x

theorem gradedMul_algebraMap (x : (â¨ i, ğ’œ i) âŠ—[R] (â¨ i, â„¬ i)) (r : R) :
    gradedMul R ğ’œ â„¬ x (algebraMap R _ r âŠ—â‚œ 1) = r â€¢ x := by
  suffices (gradedMul R ğ’œ â„¬).flip (algebraMap R _ r âŠ—â‚œ 1) = DistribMulAction.toLinearMap R _ r by
    exact DFunLike.congr_fun this x
  ext
  dsimp
  erw [tmul_of_gradedMul_of_tmul]
  rw [mul_zero, uzpow_zero, one_smul, smul_tmul',
      mul_one, _root_.Algebra.smul_def, Algebra.commutes]
  rfl

theorem gradedMul_one (x : (â¨ i, ğ’œ i) âŠ—[R] (â¨ i, â„¬ i)) :
    gradedMul R ğ’œ â„¬ x 1 = x := by
  -- Note: https://github.com/leanprover-community/mathlib4/pull/8386 had to specialize `map_one` to avoid timeouts.
  simpa only [RingHom.map_one, one_smul] using gradedMul_algebraMap ğ’œ â„¬ x 1

theorem gradedMul_assoc (x y z : DirectSum _ ğ’œ âŠ—[R] DirectSum _ â„¬) :
    gradedMul R ğ’œ â„¬ (gradedMul R ğ’œ â„¬ x y) z = gradedMul R ğ’œ â„¬ x (gradedMul R ğ’œ â„¬ y z) := by
  let mA := gradedMul R ğ’œ â„¬
    -- restate as an equality of morphisms so that we can use `ext`
  suffices LinearMap.llcomp R _ _ _ mA âˆ˜â‚— mA =
      (LinearMap.llcomp R _ _ _ LinearMap.lflip.toLinearMap <|
        LinearMap.llcomp R _ _ _ mA.flip âˆ˜â‚— mA).flip by
    exact DFunLike.congr_fun (DFunLike.congr_fun (DFunLike.congr_fun this x) y) z
  ext ixa xa ixb xb iya ya iyb yb iza za izb zb
  dsimp [mA]
  simp_rw [tmul_of_gradedMul_of_tmul, Units.smul_def, â† Int.cast_smul_eq_zsmul R,
    LinearMap.map_smulâ‚‚, LinearMap.map_smul, DirectSum.lof_eq_of, DirectSum.of_mul_of,
    â† DirectSum.lof_eq_of R, tmul_of_gradedMul_of_tmul, DirectSum.lof_eq_of, â† DirectSum.of_mul_of,
    â† DirectSum.lof_eq_of R, mul_assoc]
  simp_rw [Int.cast_smul_eq_zsmul R, â† Units.smul_def, smul_smul, â† uzpow_add, add_mul, mul_add]
  congr 2
  abel

theorem gradedComm_gradedMul (x y : DirectSum _ ğ’œ âŠ—[R] DirectSum _ â„¬) :
    gradedComm R ğ’œ â„¬ (gradedMul R ğ’œ â„¬ x y)
      = gradedMul R â„¬ ğ’œ (gradedComm R ğ’œ â„¬ x) (gradedComm R ğ’œ â„¬ y) := by
  suffices (gradedMul R ğ’œ â„¬).comprâ‚‚ (gradedComm R ğ’œ â„¬).toLinearMap
      = (gradedMul R â„¬ ğ’œ âˆ˜â‚— (gradedComm R ğ’œ â„¬).toLinearMap).complâ‚‚
        (gradedComm R ğ’œ â„¬).toLinearMap from
    LinearMap.congr_funâ‚‚ this x y
  ext iâ‚ aâ‚ jâ‚ bâ‚ iâ‚‚ aâ‚‚ jâ‚‚ bâ‚‚
  dsimp
  rw [gradedComm_of_tmul_of, gradedComm_of_tmul_of, tmul_of_gradedMul_of_tmul]
  -- Note: https://github.com/leanprover-community/mathlib4/pull/8386 had to specialize `map_smul` to avoid timeouts.
  simp_rw [Units.smul_def, â† Int.cast_smul_eq_zsmul R, LinearEquiv.map_smul, LinearMap.map_smul,
    LinearMap.smul_apply]
  simp_rw [Int.cast_smul_eq_zsmul R, â† Units.smul_def, DirectSum.lof_eq_of, DirectSum.of_mul_of,
    â† DirectSum.lof_eq_of R, gradedComm_of_tmul_of, tmul_of_gradedMul_of_tmul, smul_smul,
    DirectSum.lof_eq_of, â† DirectSum.of_mul_of, â† DirectSum.lof_eq_of R]
  simp_rw [â† uzpow_add, mul_add, add_mul, mul_comm iâ‚ jâ‚‚]
  congr 1
  abel_nf
  rw [two_nsmul, uzpow_add, uzpow_add, Int.units_mul_self, one_mul]

end TensorProduct
